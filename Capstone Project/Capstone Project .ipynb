{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Temperature and Immigration Data ETL\n",
    "\n",
    "#### Project Summary\n",
    "To build an ETL pipeline with Spark using data that Udacity had provided on Immigration and Temperature patterns in US cities. The data gets transformed in a relational data model that is optimized to analyse immigration events. The database then can be used to understand immigration patterns with respect to temperature in US cities\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The scope of the project is to create two dimensions table and one fact table. The first dimension table would be for the Immigration data and the second dimension table would be for Temperature Data. The fact table will be formed by joining on Immigration and Temperature. We will be using Spark to process the data.\n",
    "\n",
    "#### Describe and Gather Data\n",
    "The I94 immigration data comes from the US National Tourism and Trade Office website. It is provided in SAS7BDAT format which is a binary database storage format.\n",
    "\n",
    "####  Immigration Data\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. The immigration data has the following columns and I have taken from \n",
    "the knowledge section what each of them mean\n",
    "- cicid >>>> CICID is a unique number for the immigrants. (No null values found).\n",
    "- I94res>>>> is country from where one has travelled. (No null values found).\n",
    "- I94addr>>>> is where the immigrants resides in USA . (Found null values).\n",
    "- arrdate>>>> is date of arrrival . (Convert it to timestamp format).\n",
    "- visatype>>>> is the type of visa which one owns . (No null values found).\n",
    "- i94yr>>>>4 digit year\n",
    "- i94mon>>>>Numeric month\n",
    "- i94cit>>>> 3 digit code of origin city\n",
    "- i94port>>>> 3 character code of destination USA city\n",
    "- i94mode>>>> 1 digit travel code (transportation)\n",
    "- FLTNO >>>> Flight number of Airline used to arrive in U.S\n",
    "- VISATYPE >>>> Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "- ADMNUM>>> Admission Number\n",
    "- AIRLINE >>>> Airline used to arrive in U.S\n",
    "- BIRYEAR>>>> 4 digit year of birth\n",
    "- I94BIR>>>>> Age of Respondent in Years\n",
    "- DEPDATE>>>>>> the Departure Date from the USA.\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here (only for the month of April)\n",
    "immigration_data = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immi = pd.read_sas(immigration_data, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_immi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data \n",
    "\n",
    "The temperature data comes from Kaggle \n",
    "https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data comes from Kaggle.\n",
    "- dt: Date\n",
    "- AverageTemperature: Temperature data\n",
    "- Average Temperature Uncertainity: Uncertainity in the data\n",
    "- City\n",
    "- Country\n",
    "- Latitude\n",
    "- Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Reading in the temperature data\n",
    "temp_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(temp_data, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8599202</th>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>4.303</td>\n",
       "      <td>0.341</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599203</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1.479</td>\n",
       "      <td>0.217</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599204</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1.559</td>\n",
       "      <td>0.304</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599205</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>2.253</td>\n",
       "      <td>0.267</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599206</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>7.710</td>\n",
       "      <td>0.182</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599207</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>11.464</td>\n",
       "      <td>0.236</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599208</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>15.043</td>\n",
       "      <td>0.261</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599209</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>18.775</td>\n",
       "      <td>0.193</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599210</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>18.025</td>\n",
       "      <td>0.298</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "8599202  2012-12-01               4.303                          0.341   \n",
       "8599203  2013-01-01               1.479                          0.217   \n",
       "8599204  2013-02-01               1.559                          0.304   \n",
       "8599205  2013-03-01               2.253                          0.267   \n",
       "8599206  2013-04-01               7.710                          0.182   \n",
       "8599207  2013-05-01              11.464                          0.236   \n",
       "8599208  2013-06-01              15.043                          0.261   \n",
       "8599209  2013-07-01              18.775                          0.193   \n",
       "8599210  2013-08-01              18.025                          0.298   \n",
       "8599211  2013-09-01                 NaN                            NaN   \n",
       "\n",
       "           City      Country Latitude Longitude  \n",
       "8599202  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599203  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599204  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599205  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599206  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599207  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599208  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599209  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599210  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599211  Zwolle  Netherlands   52.24N     5.26E  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.tail(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Creating a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "## Create spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "I started with destination ports and cleaned them up. I understand that there are other columns that have missing values or the date column or Nan values in the gender,adress,matflag and so on. Since my analysis has to do with more destination ports and their temperature dependence, I would not be deleting rows that have Nan in them specially in columns that I have interest. I shall however look into destination ports column and the residential adress column\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i94mode',\n",
       " 'i94addr',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'dtadfile',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'fltno']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immi.columns[df_immi.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 3522 rows that doesnt have the destination port. It wouldnt help us with our data exploration\n",
    "len(df_immi[df_immi['i94port'] == 'XXX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Okay so these ports dont exist as well and there are 4215 rows\n",
    "ports_that_dont_exist = ['FRG','HRL','ISP','JSJ','BUS','IAG','PHN','STN','VMB','T01','PHF','DRV','FTB','GAC','GMT','JFA','JMZ','NC8','NYL','OAI','PCW','WA5','WTR', 'X96', 'XNA',\n",
    "'YGF','5T6', '060','SP0', 'W55','X44', 'AUH','RYY','SUS','74S','ATW','CPX','MTH','PFN','SCH','ASI', 'BKF', 'DAY', 'Y62' 'AG','BCM','DEC','PLB', 'CXO', 'JBQ', 'JIG',\n",
    "'OGS','TIW','OTS','AMT','EGE','GPI','NGL','OLM','.GA','CLX', 'CP ', 'FSC', 'NK',  'ADU','AKT','LIT', 'A2A','OSN']\n",
    "df_immi['i94port'].isin(ports_that_dont_exist).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Now i94_port has all the ports that are valid. Therefore the dictionary has all the valid ports. Used a regex format to get all such columns. The \n",
    "## explanation for regex is given below\n",
    "import re\n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94_port = {}\n",
    "with open('i94_port.txt') as f:\n",
    "     for data in f:\n",
    "             match = re_obj.search(data)\n",
    "             i94_port[match[1]]=[match[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- \\': This gets the \"'\" from the text\n",
    "- (.*)\" : This gives us the first group upto and untill the end of the letters\n",
    "- \\' : This gives us the next \"'\" from the text\n",
    "- .* : This gives us everything after until the next \"'\"\n",
    "- (.*) : This gives us the next group. i.e: The second match\n",
    "- \\' : This gives us the last '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Creating a dictionary for the residential address using regex\n",
    "re_obj = re.compile(r'\\s*(\\d.*\\d)\\s.*\\s(\\'\\w.*)')\n",
    "i94_res_add = {}\n",
    "with open('i94_res_cit.txt') as f:\n",
    "     for data in f:\n",
    "           # print(data)\n",
    "            match = re_obj.search(data)\n",
    "            i94_res_add[match[1]]=[match[2]]\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- \\s*: Every white space untill group 1\n",
    "- (\\d.*\\d): First group and it has all the digits\n",
    "- \\s.*\\s: The next bunch of white space\n",
    "- (\\'\\w.*): The second group that has the name of the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Creating a function to clean the immigration data\n",
    "def clean_immi(data,dictionary,col):\n",
    "    '''\n",
    "    Input\n",
    "    Data: The input file for cleaning\n",
    "    Dictionary: The dictionary that has the valid key-values combinations\n",
    "    '''\n",
    "    ## Read the file into spark\n",
    "    df_immi = spark.read.format('com.github.saurfang.sas.spark').load(data)\n",
    "    \n",
    "    ## Cleaning the invalid codes\n",
    "    df_immi = df_immi.filter(df_immi[col].isin(list(dictionary.keys())))\n",
    "    \n",
    "    return df_immi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|20555.0|  57.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247103803E10|00602|      B1|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  63.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1953.0|09302016|  null|  null|     AZ|9.247139923E10|00602|      B2|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20558.0|  57.0|    2.0|  1.0|20160401|    null| null|      O|      K|   null|      M| 1959.0|09302016|  null|  null|     AZ|9.247161383E10|00602|      B2|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20553.0|  46.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1970.0|09302016|  null|  null|     AZ|9.247079603E10|00602|      B2|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20562.0|  48.0|    1.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1968.0|09302016|  null|  null|     AZ|9.247848973E10|00608|      B1|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|20671.0|  52.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1964.0|09302016|  null|  null|     TK|9.250139443E10|00001|      B2|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|20554.0|  33.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1983.0|09302016|  null|  null|     MQ|9.249090503E10|03348|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|20578.0|  49.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1967.0|09302016|     M|  null|     OS|9.247020943E10|00089|      B2|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|20611.0|  43.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1973.0|09302016|     M|  null|     OS|9.247128923E10|00089|      B2|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|20554.0|  53.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1963.0|09302016|     F|  null|     TK|9.250930163E10|00033|      B2|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  48.0|    2.0|  1.0|20160401|     TIA| null|      G|   null|   null|   null| 1968.0|09302016|     M|  null|     AZ|9.247042023E10|00602|      B2|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|   null|  74.0|    2.0|  1.0|20160401|     TIA| null|      T|   null|   null|   null| 1942.0|09302016|     F|  null|     TK|  6.69712185E8|    1|      B2|\n",
      "| 36.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|20561.0|  37.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1979.0|09302016|     M|  null|     TK|9.250625823E10|00001|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "0:00:03.322631\n"
     ]
    }
   ],
   "source": [
    "## This testing if the function works\n",
    "start=datetime.now()\n",
    "immi_test = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' \n",
    "df_immi_c= clean_immi(immi_test,i94_port,\"i94port\")\n",
    "df_immi_c.show()\n",
    "print (datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3448"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clean the temperature data\n",
    "len(df_temp['City'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['City'].isnull().sum()\n",
    "## We have no null values in the City column which is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City',\n",
       "       'Country', 'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_temp (data):\n",
    "    df_temp_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load(temp_data)\n",
    "    df_temp_spark = df_temp_spark.filter(df_temp_spark.AverageTemperature != 'NaN')\n",
    "    df_temp_spark = df_temp_spark.dropDuplicates(['City', 'Country'])\n",
    "    df_temp_spark = df_temp_spark.withColumn(\"i94port\", conv_city_abbr(df_temp_spark.City))\n",
    "    df_temp_spark = df_temp_spark.filter(df_temp_spark.i94port != 'null')\n",
    "    return df_temp_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364130"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There are plenty of rows with Nan values and we would have to filter these rows. Using spark engine to filter these rows would be faster\n",
    "df_temp['AverageTemperature'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_spark = df_temp_spark.filter(df_temp_spark.AverageTemperature != 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_spark = df_temp_spark.dropDuplicates(['City', 'Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Have to remove the duplicates rows and this shows us that the dataframe reduces to 3490\n",
    "len(df_temp.drop_duplicates(subset = ['City','Country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Adding the function to add cities abbreivations as an additional column\n",
    "from pyspark.sql.functions import udf\n",
    "@udf()\n",
    "def conv_city_abbr(city):\n",
    "    '''\n",
    "    Input: City names from temp\n",
    "    Output: Corresponding i94_port from the dictionary\n",
    "    '''\n",
    "    \n",
    "    for key in i94_port:\n",
    "        if city.lower() in i94_port[key][0].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_spark = df_temp_spark.withColumn(\"i94port\", conv_city_abbr(df_temp_spark.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+--------+--------------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|    City|       Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+--------+--------------+--------+---------+-------+\n",
      "|1743-11-01|             8.758|                        1.886|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-04-01|6.0699999999999985|           2.9339999999999997|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-05-01|             7.751|                        1.494|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-06-01|             10.62|                        1.574|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-07-01|             12.35|                        1.591|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-09-01|            11.224|           1.6059999999999999|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-10-01|             8.945|           1.7309999999999999|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-11-01|             7.836|                        1.585|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1744-12-01|             5.263|                        1.871|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1745-01-01|             4.136|                        1.825|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1745-02-01|             2.436|           1.6769999999999998|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1745-03-01|              3.24|                        1.456|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1745-04-01|             4.819|                        1.429|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-01-01|             5.441|                        1.463|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-02-01|              7.31|                        2.474|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-03-01|             7.335|           3.0239999999999996|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-04-01|             6.604|                        1.308|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-05-01| 8.328000000000001|           1.5090000000000001|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-06-01|            10.803|                        1.393|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1750-07-01|            14.367|                          1.4|Aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "+----------+------------------+-----------------------------+--------+--------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_spark = df_temp_spark.filter(df_temp_spark.i94port != 'null')\n",
    "df_temp_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "The idea behind the data model is to create a fact table and dimensions table. The fact table would have all the important columns from the immigration data and the dimensions table\n",
    "would be subgroup from the immigration and the temperature data \n",
    "\n",
    "#### Fact Table:\n",
    "Columns\n",
    "- i94port = code for the USA ports (abbreivation)\n",
    "- AverageTemperature = average temperature destination city\n",
    "- i94cit = code for origin city\n",
    "\n",
    "#### Dimenstion Table: Immigration\n",
    "- i94yr = year column \n",
    "- i94mon = month column\n",
    "- i94mode = single digit travel code\n",
    "- arrdate = arrival date\n",
    "- i94visa = reason for immigration\n",
    "- i94port = code for the USA ports (This would be the joining key)\n",
    "- depdate = Departure date\n",
    "\n",
    "#### Dimension Table: Temperature\n",
    "- City = city name\n",
    "- Country = Name of the country \n",
    "- Latitude = Latitude\n",
    "- Longitude = Longitude\n",
    "- i94port = code for the USA ports (This would be the joining key)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "- Clean the I94 immigration data to remove the Null values from the City and residential address columns\n",
    "- Clean the temperature data to remove the null values from Cities and Average Temperature columns\n",
    "- Create fact table by joining immigration and temperature data and selecting the relvant columns\n",
    "- Create immigration dimension table by selecting the relevant columns\n",
    "- Create temperature dimenstion table by selecting the relevant columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Cleaning the immigration data\n",
    "immi_data = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immi = clean_immi(immi_data,i94_port,\"i94port\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Extracting the columns for immigration dimension table\n",
    "immi_table = df_immi.select([\"i94yr\", \"i94mon\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the temperature data\n",
    "temp_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = clean_temp(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty|     City|             Country|Latitude|Longitude|i94port|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "|1852-07-01|             15.488|                        1.395|    Perth|           Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|             -1.977|                        2.551|  Seattle|       United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|              2.767|                        1.905| Hamilton|              Canada|  42.59N|   80.73W|    HAM|\n",
      "|1849-01-01|  7.399999999999999|                        2.699|  Ontario|       United States|  34.56N|  116.76W|    ONT|\n",
      "|1821-11-01|              2.322|                        2.375|  Spokane|       United States|  47.42N|  117.24W|    SPO|\n",
      "|1843-01-01| 18.874000000000002|                        2.017|Abu Dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1824-01-01|             25.229|                        1.094|    Anaco|           Venezuela|   8.84N|   64.05W|    ANA|\n",
      "|1855-05-01|              9.904|           1.4369999999999998|      Ica|                Peru|  13.66S|   75.14W|    CHI|\n",
      "|1835-01-01|              9.833|                        2.182|  Nogales|       United States|  31.35N|  111.20W|    NOG|\n",
      "|1743-11-01|  8.129999999999999|                        2.245|  Atlanta|       United States|  34.56N|   83.68W|    ATL|\n",
      "|1796-01-01|             15.552|                        2.305|      Mau|               India|  26.52N|   84.18E|    OGG|\n",
      "|1743-11-01|              3.264|                        1.665|   Newark|       United States|  40.99N|   74.56W|    NEW|\n",
      "|1857-01-01| 18.581000000000003|           1.8119999999999998|  Springs|        South Africa|  26.52S|   28.66E|    PSP|\n",
      "|1856-01-01| 26.055999999999997|           1.3769999999999998|      Ise|             Nigeria|   7.23N|    5.68E|    BOI|\n",
      "|1743-11-01|             18.722|                        2.302|  Orlando|       United States|  28.13N|   80.91W|    ORL|\n",
      "|1823-01-01|             11.602|           2.8160000000000003|   Laredo|       United States|  28.13N|   99.09W|    LCB|\n",
      "|1841-01-01| 13.107999999999999|                        2.519|     Tali|              Taiwan|  24.92N|  120.59E|    MET|\n",
      "|1828-01-01|-2.7630000000000003|                        2.617| Victoria|              Canada|  49.03N|  122.45W|    VIC|\n",
      "|1743-11-01| 1.1880000000000002|                        1.531|   Boston|       United States|  42.59N|   72.00W|    BOS|\n",
      "|1849-01-01|  8.091999999999999|           2.1919999999999997|Fairfield|       United States|  37.78N|  122.03W|    FTF|\n",
      "+----------+-------------------+-----------------------------+---------+--------------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract columns for temperature dimension table\n",
    "temp_table = df_temp.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------------------+--------+---------+-------+\n",
      "| AverageTemperature|     City|             Country|Latitude|Longitude|i94port|\n",
      "+-------------------+---------+--------------------+--------+---------+-------+\n",
      "|             15.488|    Perth|           Australia|  31.35S|  114.97E|    PER|\n",
      "|             -1.977|  Seattle|       United States|  47.42N|  121.97W|    SEA|\n",
      "|              2.767| Hamilton|              Canada|  42.59N|   80.73W|    HAM|\n",
      "|  7.399999999999999|  Ontario|       United States|  34.56N|  116.76W|    ONT|\n",
      "|              2.322|  Spokane|       United States|  47.42N|  117.24W|    SPO|\n",
      "| 18.874000000000002|Abu Dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|             25.229|    Anaco|           Venezuela|   8.84N|   64.05W|    ANA|\n",
      "|              9.904|      Ica|                Peru|  13.66S|   75.14W|    CHI|\n",
      "|              9.833|  Nogales|       United States|  31.35N|  111.20W|    NOG|\n",
      "|  8.129999999999999|  Atlanta|       United States|  34.56N|   83.68W|    ATL|\n",
      "|             15.552|      Mau|               India|  26.52N|   84.18E|    OGG|\n",
      "|              3.264|   Newark|       United States|  40.99N|   74.56W|    NEW|\n",
      "| 18.581000000000003|  Springs|        South Africa|  26.52S|   28.66E|    PSP|\n",
      "| 26.055999999999997|      Ise|             Nigeria|   7.23N|    5.68E|    BOI|\n",
      "|             18.722|  Orlando|       United States|  28.13N|   80.91W|    ORL|\n",
      "|             11.602|   Laredo|       United States|  28.13N|   99.09W|    LCB|\n",
      "| 13.107999999999999|     Tali|              Taiwan|  24.92N|  120.59E|    MET|\n",
      "|-2.7630000000000003| Victoria|              Canada|  49.03N|  122.45W|    VIC|\n",
      "| 1.1880000000000002|   Boston|       United States|  42.59N|   72.00W|    BOS|\n",
      "|  8.091999999999999|Fairfield|       United States|  37.78N|  122.03W|    FTF|\n",
      "+-------------------+---------+--------------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Temporary Views for creating the fact table\n",
    "df_immi.createOrReplaceTempView(\"immigration\")\n",
    "df_temp.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the fact table by joining the immigration and temperature views\n",
    "fact_immi_temp = spark.sql('''\n",
    "SELECT immigration.i94cit as city,\n",
    "       immigration.i94port as i94port,\n",
    "       temp.AverageTemperature as temperature\n",
    "FROM immigration\n",
    "JOIN temp ON (immigration.i94port = temp.i94port)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------------+\n",
      "| city|i94port|      temperature|\n",
      "+-----+-------+-----------------+\n",
      "|111.0|    SNA|7.168999999999999|\n",
      "|114.0|    SNA|7.168999999999999|\n",
      "|117.0|    SNA|7.168999999999999|\n",
      "|129.0|    SNA|7.168999999999999|\n",
      "|575.0|    SNA|7.168999999999999|\n",
      "|575.0|    SNA|7.168999999999999|\n",
      "|577.0|    SNA|7.168999999999999|\n",
      "|577.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "|582.0|    SNA|7.168999999999999|\n",
      "+-----+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immi_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def quality(df):\n",
    "    '''\n",
    "    Input: Spark dataframe\n",
    "    Output: Print outcome of data quality check\n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"No records\")\n",
    "    else:\n",
    "        print(\"Number of records =  {} \".format(result))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records =  3088544 \n"
     ]
    }
   ],
   "source": [
    "# Perform data quality check\n",
    "quality(df_immi)\n",
    "quality(df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "### Fact Table:\n",
    "Contains a combination of immigration data and temperature data that has been combined by i94port\n",
    "\n",
    "Columns\n",
    "- i94port = code for the USA ports (abbreivation)\n",
    "- AverageTemperature = average temperature destination city\n",
    "- i94cit = code for origin city\n",
    "\n",
    "### Dimension Table: Immigration\n",
    "- i94yr = year column \n",
    "- i94mon = month column\n",
    "- i94mode = single digit travel code\n",
    "- arrdate = arrival date\n",
    "- i94visa = reason for immigration\n",
    "- i94port = code for the USA ports (This would be the joining key)\n",
    "- depdate = Departure date\n",
    "\n",
    "### Dimenstion Table: Temperature\n",
    "- City = city name\n",
    "- Country = Name of the country \n",
    "- Latitude = Latitude\n",
    "- Longitude = Longitude\n",
    "- i94port = code for the USA ports (This would be the joining key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - I used Spark for this project because of its capability to handle large sets of data in different formats (CSV,SAS,txt). The fact table was  created   using Spark SQL to read data from immigration and temperature data and join them using SQL.\n",
    " \n",
    " \n",
    "* Propose how often the data should be updated and why.\n",
    "    - Monthly because the format of the data is saved monthly\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    - Load into Redshift. It can do the heavy lifting and load data in a larger scale \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - Using Airflow we can create a trigger to run the pipeline on the previous night. Integrate the job into the airflow and run it every day the previous night\n",
    " * The database needed to be accessed by 100+ people.\n",
    "    - We would need more resources inorder to get a faster experience. By using distributed database and scaling up our resources we can get faster results for the 100+ people\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
